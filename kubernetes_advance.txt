Static Pod:

it is not managed by kubernetes master node it is manage by kubelet at the worker node.
if we want to create or delete the static pod we have to create the yaml file in a specific location 
on the worker node. we can not manage create, delete, update the static pod by master node.

-------------------------------------------------------------------------------------------------------------

Node Affinity:

It is nothing but enhance version of node selector.
here instad of using lables we can use multiple condition to 
schdule the pod on worker node.

Node affinity is used for pod allocation on worker node.
Node Anti-affinity is use not to schedule pod on node.

For example:

for simple nodeselector pod  definition:

container:
 - name: nginx
   image: nginx
nodeselector:
   disktype: ssd

container:
 - name: nginx
   image: nginx
affinity:
 nodeAffinity:
  requiredDuringSchdulingIgnoredDuringExecution:
   nodeSelectorTerms:
    - matchExpressions:
      - key: disktype
        operator: In
        value:
         - ssd

--------------------------------------------------------------------------------

Pod self healing policy:

evenif we dont mention the selfhealing policy in pod bydefault it has restart policy.
self healing policy means restart policy.

type of policy:
always
onfailure
never


----------------------------------------------------------------------------------

Service discovery:

whenever we create service/pod in cluster it attached with DNS name.
so other object want to communicate with these service we can use its name.
but if they belong to differnet name space they have use fully qualifies DNS name.


-----------------------------------------------------------------------------------
DNS in kubernetes:

whenever we create any object in kubernetes in kubernetes it gets the DNS.

DNS runs as a service in kube-system namespace.

--------------------------------------------------------------------------------------

Network Policy:

k8 network policy are used to control the traffic flow at ip address or port level.

network policy is object in kubernetes

type of netwrk policy
other pods that are allowd
namespace that are allowed
ipblock(ip range)


Network policy allows to build secure network keeping pods isolated from traffic they do not need.
byfault pods are not isolated

we can isolate the pod by applying network policy

network policy component
1.podSelector

eg.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: sample-net-policy
  namespace: default
spec:
 podSelector:
  matchLabels:
   role: front-end

Network policy apply on ingress , egress & both
...
ingress:
 - from:
   - podSelector:
      matchLabels:
       role: client
2.namespaceSelector
...
ingress:
 - from:
   - namespaceSelector:
      matchLabels:
      role: client

3.ipBlock

...
ingress:
 - from:
   - ipBlock:
      cidr: 172.17.0.0/16

Ports In network Policy:

...
ingress
 - from:
   ports:
   - protocol: TCP
     port: 80

...

----------------------------------------------------------------------------

Kubernetes monitoring with promethues & grafance

----------------------------------------------------------------------------

Container logs:

kubectl logs podname containername

We can get the logs for k8-service on each node using journalctl

sudo journelctl -u kubectl

sudo journelctl -u docker

kubernetes cluster component have log to 

/var/log/kube-apiserver.log
/var/log/kube-scheduler.log
/var/log/kube-controller-manager.log

kubeadm logs are not generated at /var/log because many component run in container

-----------------------------------------------------------------------------------------------

kubectl run podname --image=nginx

kubectl apply -f ymlfie

kubectl scale --replicas=5 rc/myrc -n grras

kubectl edit rc/myrc -n grras
kubectl create ns myns --dry-run=client -o yaml

kubectl create ns myns --dry-run=client -o yaml > ns.yaml

###replicas set working with set & equality based selecotor######
###replica set work only with equality based selector#####

Replica set:

replica:4
selector
  matchExpression:
    key:app operator:in value:[prod1,prod2]
pod template:
  label:
    app: prod2
   container:
     -name:
example:

spec:
  replicas:4
  selector:
    matchExpressions:
      - [key:app, operator:In, values:[prod1,prod2]]
  template:
    metadata:
      labels:
        app:prod2
    spec:
      containers:
        name: nginx
        image: nginx
        port:
          containerPort: 80

######deployment##################



spec:
  replicas:4
  selector:
    matchLabels:
      app: prod
  template:
    metadata:
      labels:
        app:prod
    spec:
      containers:
        name: nginx
        image: nginx
        port:
          containerPort: 80



#command to update the image
kubectl set image deployment/name nginx=nginx:1.20 -n grras --record

#to check the update status
kubectl rollout status deploy/name -n grras

#to check rollout histroy
kubectl rollout history deploy/name -n grras

#to rollout undo the deployment
kubectl rollout undo --to-revision=1 deploy/name -n grras

-------------------------------------------------------------------------------------------------------------------------------------------------


Daemon set:

we if want to deploy pod on each node then we create deamon set for it.
its deply pod on each node.
byfault master node is taint, no pod will be schdule on it. so deamonset will deploy the pod 
on worker nodes.

--------------------------------------------------------------------------------------------------------------------------------------------------

static pod:

if we want to deply the pod but we dont want to manage this my kubernetes. so we can create static pod.
this pod is not managed my kubernetes, it is created by kubelets of node.

to create static pod, we have to create a yml file at perticular place in the node.


--------------------------------------------------------------------------------------------------------

How to control pod schduling:


If we want to schdule the node on specific node, there are two way to schdule the pod on perticular node.

1.nodeName:in this we have to mention the node
2.nodeSelector: in this we have to mention the tag

example:
apiVersion: v1
kind Pod
metadata:
  name: web1
  namespace: grras
spec:
  nodeName: node1
  container:
    - image: nginx
      name: con1

apiVersion: apps/v1
kind: Deployment
metadata:
  name: flip
  namespace: grras
spec:
  replicas: 4
  selector:
    matchLabels:
      app: prod
  template:
    metadata:
      labels:
        app: prod
    spec:
      nodeName: node1
      containers:
        - name: nginx
          image: nginx
          

note: make sure its priority is high as compair cordon
note: if node selector/nodename dont found any node it will go in pending state

